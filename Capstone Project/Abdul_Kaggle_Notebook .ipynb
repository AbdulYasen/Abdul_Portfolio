{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Capstone Project \n",
    "\n",
    "## AutoTel Shared Cars Availability\n",
    "\n",
    "### Location history of shared cars\n",
    "\n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In order to reduce the number of owned cars, the city of Tel Aviv launched a shared-car project, called AutoTel. Users of the service are able to reserve a car using a mobile app, and pay for it by the minute. The project that was launched in October 2017 attracted over 7500 users, with more than 50% of them using the service at least once a week.\n",
    "\n",
    "\n",
    "From the AutoTel website we extracted the location of the parked cars, every two minutes for several months. The raw data was saved to Google Storage in CSV format, and later loaded to a BigQuery Table. This short clip shows a visualization of the recorded data using Uber’s kepler.gl tool.\n",
    "\n",
    "To select from the BQ table run:\n",
    "\n",
    "select * from `gad-playground-212407.doit_intl_autotel_public.car_locations` LIMIT 1\n",
    "Inspiration\n",
    "In order for the service to be reliable, AutoTel has to make sure that supply and demand are geospatially balanced, meaning cars are where and when they are needed. This task is extremely difficult since cars are driven and parked by customers who are not aligned at all with this optimization task. For the most part, the distribution of cars is uncorrelated with the demand: one reason is that if a car is parked in a suburban neighborhood, it may take a long time before another user may drive it to the city center, where high demand for the cars exists; thus clusters of unused cars are very often present on the outskirts of the city.\n",
    "\n",
    "Using machine learning, AutoTel can predict the geospatial availability of cars at given times, and use predictions to modify their business model. They could, for example, modify prices so that it would be cheaper to park cars in high demand areas, or plan the the maintenance program so that cars will be collected from high-supply-low-demand areas and returned to areas of high demand.\n",
    "\n",
    "The data sources is: https://www.kaggle.com\n",
    "\n",
    "---\n",
    "\n",
    "The part 2,3 of capstone project is focused on exploratory data analysis, aka \"EDA\". EDA is an essential part of the data science analysis pipeline. Failure to perform EDA before modeling is almost guaranteed to lead to bad models and faulty conclusions.\n",
    "\n",
    "#### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-a96ee40bba8a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-a96ee40bba8a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://www.kaggle.com/acacianinjayt/abdul-notebook/edit/run/35488683\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# MY kaggle notebook\n",
    "# the lightgbm model dosent work on my local jupyter notebook, \n",
    "https://www.kaggle.com/acacianinjayt/abdul-notebook/edit/run/35488683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings\n",
    "\n",
    "# this line tells jupyter notebook to put the plots in the notebook rather than saving them to file.\n",
    "%matplotlib inline\n",
    "\n",
    "# this line makes plots prettier on mac retina screens. If you don't have one it shouldn't do anything.\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Load the `2020_02_25.csv` dataset and describe it\n",
    "\n",
    "---\n",
    "\n",
    "I got the `2020_02_25.csv` dataset from The data sources https://www.kaggle.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars = pd.read_csv('../input/autotel-shared-car-locations/2020_02_25.csv')\n",
    "df_cars.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample = df_cars.sample(n=100000)\n",
    "#df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#def num(x):\n",
    "    \n",
    "    #try:\n",
    "        #return x[0]\n",
    "    #except:\n",
    "        #return np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample.cars_list.apply(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample.sample(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the rows indicate empty parking spots. These spots are reserved for AutoTel cars, but no car was parked there at the time. Since we are only interested in the cars location, we will filter these rows to save computation and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can filter the unwanted rows \n",
    "\n",
    "df = df_cars[df_cars['total_cars'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape of data frame show us half of rows filtered out\n",
    "# we can save computation and memory by this way\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by aggregate by one or more columns in pandas\n",
    "# The example of to group on one or multiple columns and summarise data with aggregation function using Pandas.\n",
    "# The source from https://jamesrledoux.com/code/group-by-aggregate-pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas comes with a whole host of sql-like aggregation functions we can apply when grouping on one or more columns. This is Python’s closest equivalent to dplyr’s group_by + summarise logic. Here’s a quick example of how to group on one or multiple columns and summarise data with aggregation functions using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# reset index to get grouped column\n",
    "\n",
    "df_cars_by_time = df.groupby('timestamp').agg({'total_cars': 'sum'}\n",
    "                                          ).reset_index()\n",
    "df_cars_by_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a datetime index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kde: bool, optional - Whether to plot a gaussian kernel density estimate.\n",
    "# bins: argument for matplotlib hist(), or None, optional - Specification of hist bins. If unspecified, as reference rule is used that tries to find a useful default.\n",
    "# rug: bool, optional - Whether to draw a rugplot on the support axis.\n",
    "import seaborn as sns, numpy as np\n",
    "\n",
    "scores_dist1 = sns.distplot(df_cars_by_time.total_cars, bins=15, kde=False, rug=True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Analyzing time series by Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time series analysis\n",
    "# I need more practice for time serice \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_by_time['total_cars'].plot(lw=1.5, figsize=(12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_by_time['timestamp'] = df_cars_by_time['timestamp'].apply(pd.Timestamp)\n",
    "rolling_mean = df_cars_by_time.set_index('timestamp').sort_index().rolling(window=2, center=True).mean()\n",
    "exp_mean = df_cars_by_time.set_index('timestamp').sort_index().ewm(span=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = rolling_mean.plot(lw=1.5, figsize=(14,7))\n",
    "exp_mean.plot(ax=ax, lw=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the rolling mean is the mean of a moving window across the time periods.\n",
    "#Pandas has a lot of functionality to create rolling statistics which we will only scratch the surface of\n",
    "#There is a rolling() function that has the statistical function chained to it\n",
    "#Extract the dates from the index as timestamps.\n",
    "#the .to_timestamp() function lets you extract the timestamps.\n",
    "\n",
    "\n",
    "df_cars_by_time['timestamp'] = df_cars_by_time['timestamp'].apply(pd.Timestamp)\n",
    "df_cars_by_time.set_index('timestamp').sort_index().rolling('60min').mean().plot(figsize=(20,6), c='salmon', lw=1.6)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that tge max available cars is 260, so we can assume that this is the total number of cars available in AutoTel\n",
    "\n",
    "# By assuming this we will calculate the usage rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_by_time['usage_rate'] = (260 - df_cars_by_time['total_cars'])/260\n",
    "usage_rate = df_cars_by_time.set_index('timestamp').sort_index()['usage_rate'].rolling('60min').mean()\n",
    "usage_rate.plot(figsize=(20,6), c='mediumslateblue', lw=1.5)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage growing or decreasing over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_by_time['usage_rate'] = (260 - df_cars_by_time['total_cars'])/260\n",
    "df_cars_by_time.set_index('timestamp').sort_index()['usage_rate'].rolling('3D').mean().plot(figsize=(20,6), c='navy', lw=1.6)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyzing Usage Patterns by Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to analyze the data by time we need to consider the time zone,  \n",
    "# Tel Aviv is not in UTC timezone. \n",
    "# We will converted the capital city time zone as same time zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timezone\n",
    "\n",
    "timestamps = pd.DatetimeIndex(df_cars_by_time['timestamp'])\n",
    "timestamps = timestamps.tz_convert('Asia/Jerusalem')\n",
    "\n",
    "df_cars_by_time['Local_time'] = timestamps\n",
    "\n",
    "# extract time features\n",
    "df_cars_by_time['weekday'] = df_cars_by_time['Local_time'].dt.day\n",
    "df_cars_by_time['hour'] = df_cars_by_time['Local_time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_by_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze usage and viselize by hour\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "#plt.subplot(121)\n",
    "sns.barplot(x='hour', y='total_cars', data=df_cars_by_time)\n",
    "plt.title('Total_Cars usage by hour of day')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze usage and viselize by day\n",
    "plt.figure(figsize=(12,6))\n",
    "#plt.subplot(122)\n",
    "sns.boxplot(x='weekday', y='total_cars', data=df_cars_by_time) #showfliers=False\n",
    "plt.title('Total_Cars usage by day of week')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see that tge max available cars is 260, \n",
    "#so we can assume that this is the total number of cars available in AutoTel.\n",
    "#By assuming this we will calculate the usage rate.\n",
    "\n",
    "df_cars_by_time['usage_rate'] = (260 - df_cars_by_time['total_cars'])/260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "#plt.subplot(121)\n",
    "sns.barplot(x='hour', y='usage_rate', data=df_cars_by_time)\n",
    "plt.title('Cars usage rate by hour of day')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze usage and viselize by day\n",
    "plt.figure(figsize=(12,6))\n",
    "#plt.subplot(122)\n",
    "sns.boxplot(x='weekday', y='usage_rate', data=df_cars_by_time) #showfliers=False\n",
    "plt.title('Cars usage rate by day of week')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".    Looks much better isn't it? during the night the usage drops to as much as 2.5%, while at peak times almost 20% of the cars are in use.\n",
    "\n",
    ".    Weekdays in israel are Sunday to Thursday, the weekend is Friday and Saturday. This may explain why Thursday has the highest usage rate on average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data on a map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This geographical data can be presented on a map, showing where people park the cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive maps with the folium package\n",
    "import folium\n",
    "from folium import Choropleth, Circle, Marker\n",
    "from folium.plugins import HeatMap, MarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Along the way, I'll apply my new skills to visualize AutoTel data.\n",
    "#We add some data to the map, Location sets the map\n",
    "\n",
    "df_locations = df.groupby(['latitude', 'longitude', 'timestamp']).sum().reset_index().sample(1500)\n",
    "df_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map with folium.Map()\n",
    "m = folium.Map([df_locations.latitude.mean(), df_locations.longitude.mean()], zoom_start=11)\n",
    "for index, row in df_locations.iterrows(): # Add points to the map\n",
    "    Marker([row['latitude'], row['longitude']], radius=row['total_cars']*6, fill_color=\"#3db7e4\").add_to(m)\n",
    "    \n",
    "points = df_locations[['latitude', 'longitude']]\n",
    "m.add_children(HeatMap(points, radius=15)) #plot heatmap\n",
    "\n",
    "#Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the number of available cars per neighborhood "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can get the AutoTel provide additional data of neighborhood polygons of the city of Tel Aviv. This data will enable us to group the data by neighborhood and later predict the number of aviliable cars per neighborhood!\n",
    "    \n",
    "    . Disclaimer: Kaggle Kernels do not support GeoPandas, so I implemented my own geo joins wich are not efficient. Hopefully they will add it soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary geometric objects from shapely module\n",
    "#wkt stands for Well-Known Text and is a text markup language for representing vector geometry objects on a map,\n",
    "#wkt spatial reference systems of spatial objects and transformations between spatial reference systems.\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load neiborhood data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighborhood = pd.read_csv('/kaggle/input/tel-aviv-neighborhood-polygons/tel_aviv_neighborhood.csv')\n",
    "df_neighborhood.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_close_polygon(wkt_text):\n",
    "    poly = wkt.loads(wkt_text)\n",
    "    point_list = poly.exterior.coords[:]\n",
    "    point_list.append(point_list[0])\n",
    "    \n",
    "    return Polygon(point_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets transform the WKS's to Polygon Objects and save it to a GeoPandas DataFrame\n",
    "df_neighborhood['polygon'] = df_neighborhood['area_polygon'].apply(load_and_close_polygon)\n",
    "neighborhood_map = df_neighborhood.set_index('neighborhood_name')['polygon'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use sample(10000) data from original data\n",
    "#have many points and just one polygon and I try to find out which one of them is inside the polygon\n",
    "#need to iterate over the points and check one at a time if it is within() the polygon specified\n",
    "#if have many polygons and just one point and you want to find out which polygon contains the point\n",
    "#need to iterate over the polygons until you find a polygon that contains() \n",
    "#the point specified (assuming there are no overlapping polygons)\n",
    "sample_df = df.sample(10000)\n",
    "sample_df['points'] = sample_df.apply(lambda row : Point([row['longitude'], row['latitude']]), axis=1)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the neighborhood column\n",
    "\n",
    "poly_idxs = sample_df['points'].apply(lambda point : np.argmax([point.within(polygon) \n",
    "                                                                  for polygon in list(neighborhood_map.values())]))\n",
    "\n",
    "#In order to put points on the map, we need to convert each coordinate to geopoint, same for the dataframe.\n",
    "#keys() method in Python Dictionary, returns a view object that displays a list of all the keys in the dictionary.\n",
    "poly_idxs = poly_idxs.apply(lambda x: list(neighborhood_map.keys())[x])\n",
    "sample_df['neighborhood'] = poly_idxs.values\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "sns.barplot(x = 'neighborhood', y = 'total_cars', data=sample_df.groupby('neighborhood').count().reset_index())\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sat['geome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Car Availability using Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a copy of this object’s indices and data.\n",
    "#A shallow copy constructs a new compound object \n",
    "#and then (to the extent possible) inserts references into it to the objects found in the original.\n",
    "df_sample = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timestaps = pd.DataFrame()\n",
    "\n",
    "#Pandas drop_duplicates() method helps in removing duplicates from the data frame.\n",
    "df_timestaps['timestamp'] = df_sample.timestamp.drop_duplicates()\n",
    "\n",
    "#Manipulating and converting date times with timezone information\n",
    "#A timezone that has a variable offset from UTC.\n",
    "#Localize tz-naive DatetimeIndex to a given time zone, or remove timezone from a tz-aware DatetimeIndex.\n",
    "#timestamps = pd.DatetimeIndex(df_timestaps['timestamp']).tz_localize('UTC')\n",
    "\n",
    "#Time zone for time. \n",
    "#Corresponding timestamps would be converted to this time zone of the Datetime Array/Index. \n",
    "#A tz of None will convert to UTC and remove the timezone information.\n",
    "df_timestaps['Local_time'] = timestamps.tz_convert('Asia/Jerusalem')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The join is done on columns, the DataFrame indexes will be ignored.\n",
    "#left: use only keys from left frame\n",
    "df_sample = df_sample.merge(df_timestaps, on='timestamp', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again no reason to calculate on duplicate points, it's very expensive!\n",
    "#Creating a GeoDataFrame from a DataFrame with coordinates\n",
    "df_points = df_sample[['longitude', 'latitude']].drop_duplicates()\n",
    "\n",
    "#consider a DataFrame containing cities and their respective longitudes and latitudes\n",
    "df_points['points'] = df_points.apply(lambda row : Point([row['longitude'], row['latitude']]), axis=1)\n",
    "\n",
    "#have many points and just one polygon and I try to find out which one of them is inside the polygon\n",
    "#need to iterate over the points and check one at a time if it is within() the polygon specified\n",
    "#if have many polygons and just one point and you want to find out which polygon contains the point\n",
    "#need to iterate over the polygons until you find a polygon that contains() \n",
    "#the point specified (assuming there are no overlapping polygons)\n",
    "poly_idxs = df_points['points'].apply(lambda point : np.argmax([point.within(polygon) \n",
    "                                                                for polygon in list(neighborhood_map.values())]))\n",
    "\n",
    "#In order to put points on the map, we need to convert each coordinate to geopoint, same for the dataframe.\n",
    "#keys() method in Python Dictionary, returns a view object that displays a list of all the keys in the dictionary.\n",
    "poly_idxs = poly_idxs.apply(lambda x: list(neighborhood_map.keys())[x])\n",
    "df_points['neighborhood'] = poly_idxs.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The join is done on columns, the DataFrame indexes will be ignored.\n",
    "#left: use only keys from left frame\n",
    "df_sample = df_sample.merge(df_points[['longitude', 'latitude', 'neighborhood']], \n",
    "                            on=['longitude', 'latitude'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timestamp is just unix time with nanoseconds (so divide it by 10**6)\n",
    "df_sample['time_in_seconds'] = pd.to_datetime(df_sample['Local_time']).values.astype(np.int64) // 10**6\n",
    "\n",
    "#Generate n rows of random 24-hour times (seconds past midnight)\n",
    "seconds_in_day = 24 * 60 * 60\n",
    "seconds_in_week = 7 * seconds_in_day\n",
    "\n",
    "#the two-feature transformation in 2D as a 24-hour clock. \n",
    "#The distance between two points corresponds to the difference in time as we expect from a 24-hour cycle.\n",
    "#we will create two new features, deriving a sine transform and cosine transform of the seconds-past-midnight feature. \n",
    "df_sample['sin_time_day'] = np.sin(2*np.pi*df_sample['time_in_seconds']/seconds_in_day)\n",
    "df_sample['cos_time_day'] = np.cos(2*np.pi*df_sample['time_in_seconds']/seconds_in_day)\n",
    "#This gives you a cyclical embedding of the datetime component. \n",
    "#Thus, (e.g.) midnight and 1 am will have a similar representation\n",
    "df_sample['sin_time_week'] = np.sin(2*np.pi*df_sample['time_in_seconds']/seconds_in_week)\n",
    "df_sample['cos_time_week'] = np.cos(2*np.pi*df_sample['time_in_seconds']/seconds_in_week)\n",
    "#We can feed the sin_time and cos_time features into our machine learning model, \n",
    "#and the cyclical nature of 24-hour time will carry over.\n",
    "\n",
    "df_sample['weekday'] = df_sample['Local_time'].dt.weekday\n",
    "df_sample['hour'] = df_sample['Local_time'].dt.hour\n",
    "\n",
    "df_sample.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does our data contain multiple parking spots per Neighborhood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e multiple rounded latLongs per neighborhood?\n",
    "\n",
    "We may want to roundup the latlongs , in case reporting comes from the car level, rather than the parking spot(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sample[['longitude', 'latitude', 'neighborhood']].groupby('neighborhood').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round a DataFrame to a variable number of decimal places.\n",
    "#By providing an integer each column is rounded to the same number of decimal places\n",
    "df_sample[\"LL2\"] = df_sample['longitude'].round(2).astype(str) + df_sample['latitude'].round(2).astype(str)\n",
    "\n",
    "df_sample[['LL2', 'neighborhood']].groupby('neighborhood').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that looking at the \"parking lot\" level would mean roughly Tripling + the amount of rows/samples in our data to predict on. This might be a bit too much, although it would be more relevant for the level of taking action, i.e \"where are there missing cars + a demand for cars\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to merging + aggregation by Neighborhood:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changed : we will look at the hourly level, not minute level. (Could also do every half hour maybe?)\n",
    "\n",
    "Alternative target: Per \"parking lot\" = by LatLong2 LL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation by Neighbourhood\n",
    "aggs = {}\n",
    "aggs['total_cars'] = 'sum'\n",
    "aggs['sin_time_day'] = 'mean'\n",
    "aggs['cos_time_day'] = 'mean'\n",
    "aggs['sin_time_week'] = 'mean'\n",
    "aggs['cos_time_week'] = 'mean'\n",
    "aggs['weekday'] = 'first'\n",
    "aggs['hour'] = 'first'\n",
    "# Mode is problematic with agg\n",
    "aggs['latitude'] =   'first' # pd.Series.mode()#lambda x: x.mode #pd.Series.mode()#\n",
    "aggs['longitude'] =  'first' # pd.Series.mode() #lambda x: x.mode #pd.Series.mode() # 'first'\n",
    "\n",
    "# 30 minute resample\n",
    "df_sample = df_sample.set_index('Local_time').groupby([pd.Grouper(freq='1800s'), 'neighborhood']).agg(aggs).reset_index()\n",
    "\n",
    "# df_sample.set_index('local_time').groupby([pd.Grouper(freq='60s'), 'neighborhood']).agg(aggs).reset_index().tail()\n",
    "\n",
    "print(df_sample.shape)\n",
    "\n",
    "df_sample.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesRegressor\n",
    "\n",
    "from sklearn.model_selection  import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score, classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to categorical type\n",
    "df_sample['neighborhood'] = df_sample['neighborhood'].astype('category')\n",
    "df_sample['weekday'] = df_sample['weekday'].astype('category')\n",
    "df_sample['hour'] = df_sample['hour'].astype('category')\n",
    "\n",
    "# if there haven't label column we can ceate the lable as blow:\n",
    "\n",
    "median_total_cars = df_sample['total_cars'].median()\n",
    "\n",
    "#y is a category that the mapping function predicts\n",
    "#create lables from salary\n",
    "\n",
    "df_sample['total_cars_label'] = df_sample['total_cars'].apply(lambda x: 1 if x > median_total_cars else 0)\n",
    "\n",
    "df_sample['total_cars_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "\n",
    "X = df_sample[['neighborhood', 'weekday', 'hour']]\n",
    "y = df_sample['total_cars_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummy = pd.get_dummies(X, drop_first=True)\n",
    "print (X_dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensamble Methodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df_sample['total_cars_label'].value_counts(normalize=True))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dummy, y, test_size=0.33, \n",
    "                                                    random_state=42, stratify = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(dt, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from io import StringIO  \n",
    "#from IPython.display import Image  \n",
    "#from sklearn.tree import export_graphviz\n",
    "#import pydotplus as pydot\n",
    "\n",
    "#dot_data = StringIO()  \n",
    "\n",
    "#export_graphviz(dt, out_file=dot_data,  \n",
    "#                filled=True, rounded=True,\n",
    "#                special_characters=True)\n",
    "             \n",
    "\n",
    "#graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "#Image(graph.create_png()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dt_predict = dt.predict(X_test)\n",
    "\n",
    "print (classification_report(y_test, dt_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = X_train.sample(replace=True, n=X_train.shape[0], random_state=42)\n",
    "y_sample = y_train[X_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_tree = DecisionTreeClassifier()\n",
    "bt_tree.fit(X_sample, y_sample)\n",
    "bt_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_predict = bt_tree.predict(X_test)\n",
    "print(classification_report(y_test, bt_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = BaggingClassifier(n_estimators=10)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "bag_predict = bag.predict(X_test)\n",
    "bag.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, bag_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random ForestClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=1000)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_predict = rfc.predict(X_test)\n",
    "accuracy_score(y_test, rfc_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, rfc_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Car Availability using LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets imagine that we'd like to generate predictions to how cars will be distributed between neighborhoods in the city. So in this sample code we will try to use LightGBM to predict the number of avilable car in a neighborhood "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge lightgbm\n",
    "#conda install -c conda-forge/label/cf201901 lightgbm\n",
    "#conda install -c conda-forge/label/cf202003 lightgbm\n",
    "#pip install setuptools numpy scipy scikit-learn -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as pl\n",
    "import gc\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timestamps = pd.DataFrame()\n",
    "df_timestamps['timestamp'] = df_sample.timestamp.drop_duplicates()\n",
    "#timestamps = pd.DatetimeIndex(df_timestamps['timestamp']).tz_localize('UTC')\n",
    "df_timestamps['Local_time'] = timestamps.tz_convert('Asia/Jerusalem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.merge(df_timestamps, on='timestamp', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again no reason to calculate on duplocate points, it's very expensive!\n",
    "df_points = df_sample[['longitude','latitude']].drop_duplicates()\n",
    "df_points['points'] = df_points.apply(lambda row : Point([row['longitude'], row['latitude']]), axis=1)\n",
    "poly_idxs = df_points['points'].apply(lambda point : np.argmax([point.within(polygon) for polygon in list(neighborhood_map.values())]))\n",
    "poly_idxs = poly_idxs.apply(lambda x: list(neighborhood_map.keys())[x])\n",
    "df_points['neighborhood'] = poly_idxs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.merge(df_points[['longitude', 'latitude', 'neighborhood']], on=['longitude', 'latitude'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['time_in_seconds'] = pd.to_datetime(df_sample['Local_time']).values.astype(np.int64) // 10**6\n",
    "\n",
    "seconds_in_day = 24 * 60 * 60\n",
    "seconds_in_week = 7 * seconds_in_day\n",
    "\n",
    "#df_sample['sin_time_day'] = np.sin(2*np.pi*df_sample['time_in_seconds']/seconds_in_day)\n",
    "#df_sample['cos_time_day'] = np.cos(2*np.pi*df_sample['time_in_seconds']/seconds_in_day)\n",
    "\n",
    "#df_sample['sin_time_week'] = np.sin(2*np.pi*df_sample['time_in_seconds']/seconds_in_week)\n",
    "#df_sample['cos_time_week'] = np.cos(2*np.pi*df_sample['time_in_seconds']/seconds_in_week)\n",
    "\n",
    "df_sample['weekday'] = df_sample['Local_time'].dt.weekday\n",
    "df_sample['hour'] = df_sample['Local_time'].dt.hour\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation by Neighbourhood\n",
    "aggs = {}\n",
    "aggs['total_cars'] = 'sum'\n",
    "#aggs['sin_time_day'] = 'mean'\n",
    "#aggs['cos_time_day'] = 'mean'\n",
    "#aggs['sin_time_week'] = 'mean'\n",
    "#aggs['cos_time_week'] = 'mean'\n",
    "aggs['weekday'] = 'first'\n",
    "aggs['hour'] = 'first'\n",
    "# Mode is problematic with agg\n",
    "aggs['latitude'] =   'first' # pd.Series.mode()#lambda x: x.mode #pd.Series.mode()#\n",
    "aggs['longitude'] =  'first' # pd.Series.mode() #lambda x: x.mode #pd.Series.mode() # 'first'\n",
    "\n",
    "# 30 minute resample\n",
    "df_sample = df_sample.set_index('Local_time').groupby([pd.Grouper(freq='1800s'), 'neighborhood']).agg(aggs).reset_index()\n",
    "\n",
    "# df_sample.set_index('local_time').groupby([pd.Grouper(freq='60s'), 'neighborhood']).agg(aggs).reset_index().tail()\n",
    "\n",
    "print(df_sample.shape)\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to categorical type\n",
    "df_sample['neighborhood'] = df_sample['neighborhood'].astype('category')\n",
    "df_sample['weekday'] = df_sample['weekday'].astype('category')\n",
    "df_sample['hour'] = df_sample['hour'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_sample[df_sample['Local_time'] < '2019-01-04']\n",
    "df_test = df_sample[df_sample['Local_time'] >= '2019-01-04']\n",
    "\n",
    "print('train_shape: ', df_train.shape)\n",
    "print('test_shape: ', df_test.shape)\n",
    "\n",
    "#df_train.to_csv(\"autoTel_train_30m_Neighborhoods.csv.gz\",index=False,compression=\"gzip\")\n",
    "#df_test.to_csv(\"autoTel_test_30m_Neighborhoods.csv.gz\",index=False,compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 'total_cars'\n",
    "X = ['neighborhood', 'weekday', 'hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = lgb.LGBMRegressor(num_leaves=31,\n",
    "                        learning_rate=0.05, \n",
    "                        n_estimators=250)\n",
    "\n",
    "gbm.fit(df_train[X], df_train[y],\n",
    "        eval_set=[(df_test[X], df_test[y])],\n",
    "        eval_metric='mse',\n",
    "        early_stopping_rounds=5,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = df_test['prediction'] = gbm.predict(df_test[X])\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.plot(kind='scatter', x='total_cars', y='prediction', lw=0, s=0.4, figsize=(20,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to categorical type\n",
    "df_sample['neighborhood'] = df_sample['neighborhood'].astype('category')\n",
    "df_sample['weekday'] = df_sample['weekday'].astype('category')\n",
    "df_sample['hour'] = df_sample['hour'].astype('category')\n",
    "\n",
    "# if there haven't label column we can ceate the lable as blow:\n",
    "\n",
    "median_total_cars = df_sample['total_cars'].median()\n",
    "\n",
    "#y is a category that the mapping function predicts\n",
    "#create lables from salary\n",
    "\n",
    "df_sample['total_cars_label'] = df_sample['total_cars'].apply(lambda x: 1 if x > median_total_cars else 0)\n",
    "\n",
    "df_sample['total_cars_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sample[['neighborhood', 'weekday', 'hour']]\n",
    "y = df_sample['total_cars_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummy = pd.get_dummies(X, drop_first=True)\n",
    "print (X_dummy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dummy, y, test_size=0.33, \n",
    "                                                    random_state=42, stratify = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LGBMClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=30,\n",
    "    colsample_bytree=.8,\n",
    "    subsample=.9,\n",
    "    max_depth=7,\n",
    "    reg_alpha=.1,\n",
    "    reg_lambda=.1,\n",
    "    min_split_gain=.01,\n",
    "    min_child_weight=2,\n",
    "    silent=-1,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train, \n",
    "    eval_set= [(X_train, y_train), (X_test, y_test)], \n",
    "    eval_metric='auc', verbose=100, early_stopping_rounds=30  #30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain 10000 examples from the validation set\n",
    "# each row is an explanation for a sample, and the last column in the base rate of the model\n",
    "# the sum of each row is the margin (log odds) output of the model for that sample\n",
    "shap_values = shap.TreeExplainer(clf.booster_).shap_values(X_test.iloc[:10000,:])\n",
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the global importance of each feature as the mean absolute value\n",
    "# of the feature's importance over all the samples\n",
    "global_importances = np.abs(shap_values).mean(0)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test.iloc[:10000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
